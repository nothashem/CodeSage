---
description: 
globs: 
alwaysApply: false
---
# Testing Standards and Requirements

## Current Testing Issues

### Inadequate Coverage
- [tests/test_tokenizer.py](mdc:tests/test_tokenizer.py) only covers basic functionality
- Missing edge cases and error conditions
- No integration tests between components
- No performance benchmarks

### Missing Test Categories
- No tests for [src/bpe_tokenizer.py](mdc:src/bpe_tokenizer.py)
- No tests for training scripts
- No tests for model-tokenizer integration
- No tests for error handling scenarios

## Testing Requirements

### 1. Unit Test Coverage

#### Tokenizer Tests
**Files to test**:
- [src/tokenizer.py](mdc:src/tokenizer.py) - All methods and edge cases
- [src/bpe_tokenizer.py](mdc:src/bpe_tokenizer.py) - Complete test suite
- [src/tokenizer_cli.py](mdc:src/tokenizer_cli.py) - CLI functionality

**Required test cases**:
```python
# Edge cases for encode method
def test_encode_empty_string():
def test_encode_very_long_string():
def test_encode_unicode_characters():
def test_encode_special_characters():

# Error handling
def test_encode_invalid_input_type():
def test_load_vocab_nonexistent_file():
def test_save_vocab_permission_error():

# Tokenizer-specific tests
def test_wordpiece_subword_tokenization():
def test_bpe_merge_operations():
def test_vocabulary_consistency():
```

#### Model Tests
**Files to test**:
- [src/model/transformer.py](mdc:src/model/transformer.py) - All model components
- [src/model/layers.py](mdc:src/model/layers.py) - Individual layers
- [src/model/config.py](mdc:src/model/config.py) - Configuration validation

**Required test cases**:
```python
# Model functionality
def test_transformer_forward_pass():
def test_attention_mask_handling():
def test_causal_mask_generation():
def test_generation_with_different_sampling():

# Edge cases
def test_model_with_empty_batch():
def test_model_with_maximum_sequence_length():
def test_model_memory_usage():
```

### 2. Integration Tests

#### Tokenizer-Model Integration
Create integration tests to verify:
- Tokenizer output compatibility with model input
- End-to-end text processing pipeline
- Vocabulary size consistency between tokenizer and model

**Test file**: `tests/test_integration.py`
```python
def test_tokenizer_model_pipeline():
    """Test complete pipeline from text to model output."""
    pass

def test_vocabulary_consistency():
    """Test that tokenizer and model use same vocabulary."""
    pass

def test_training_pipeline():
    """Test complete training pipeline."""
    pass
```

#### Training Script Integration
Test the training scripts with:
- Small synthetic datasets
- Different tokenizer configurations
- Error recovery scenarios

### 3. Performance Tests

#### Tokenization Performance
Benchmark tokenization speed and memory usage:
```python
def test_tokenization_speed():
    """Benchmark tokenization speed on large text."""
    pass

def test_memory_usage():
    """Test memory usage with large vocabularies."""
    pass

def test_concurrent_tokenization():
    """Test tokenization with multiple threads."""
    pass
```

#### Model Performance
Benchmark model inference and training:
```python
def test_inference_speed():
    """Benchmark model inference speed."""
    pass

def test_training_memory_usage():
    """Test memory usage during training."""
    pass

def test_generation_speed():
    """Benchmark text generation speed."""
    pass
```

### 4. Error Handling Tests

#### Input Validation
Test all public methods with invalid inputs:
```python
def test_invalid_input_types():
def test_out_of_range_values():
def test_malformed_data():
def test_missing_files():
```

#### Recovery Scenarios
Test error recovery and graceful degradation:
```python
def test_corrupted_vocabulary_recovery():
def test_partial_training_recovery():
def test_memory_error_handling():
```

## Test Structure and Organization

### Directory Structure
```
tests/
├── unit/
│   ├── test_tokenizer.py
│   ├── test_bpe_tokenizer.py
│   ├── test_model.py
│   └── test_layers.py
├── integration/
│   ├── test_tokenizer_model.py
│   ├── test_training_pipeline.py
│   └── test_end_to_end.py
├── performance/
│   ├── test_tokenization_performance.py
│   ├── test_model_performance.py
│   └── benchmarks.py
├── fixtures/
│   ├── sample_data.py
│   ├── mock_tokenizers.py
│   └── test_configs.py
└── conftest.py
```

### Test Configuration
Use pytest configuration for:
- Test discovery and organization
- Coverage reporting
- Performance benchmarking
- Parallel test execution

### Fixtures and Utilities
Create reusable test fixtures:
```python
@pytest.fixture
def sample_texts():
    """Provide sample texts for testing."""
    return ["hello world", "test text", "unicode: 你好世界"]

@pytest.fixture
def mock_tokenizer():
    """Provide mock tokenizer for testing."""
    pass

@pytest.fixture
def small_model():
    """Provide small model for testing."""
    pass
```

## Coverage Requirements

### Minimum Coverage Targets
- **Unit tests**: 90% line coverage
- **Integration tests**: 80% integration coverage
- **Error handling**: 100% error path coverage

### Coverage Reporting
- Use pytest-cov for coverage reporting
- Generate HTML coverage reports
- Track coverage trends over time

## Continuous Integration

### Automated Testing
- Run all tests on every commit
- Run performance tests on schedule
- Generate coverage reports automatically

### Test Environments
- Test on multiple Python versions
- Test on different operating systems
- Test with different dependency versions

## Performance Benchmarks

### Benchmark Suites
Create benchmark suites for:
- Tokenization speed (tokens/second)
- Model inference speed (samples/second)
- Memory usage (MB)
- Training throughput (samples/second)

### Benchmark Tracking
- Track performance over time
- Detect performance regressions
- Compare against baseline models

## Documentation

### Test Documentation
- Document test purpose and methodology
- Explain test data and fixtures
- Document performance benchmarks

### Maintenance
- Keep tests up to date with code changes
- Remove obsolete tests
- Refactor tests for maintainability
