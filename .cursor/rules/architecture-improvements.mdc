---
description: 
globs: 
alwaysApply: false
---
# Architecture Improvements and Design Patterns

## Current Architecture Issues

### Tight Coupling
- [src/model/transformer.py](mdc:src/model/transformer.py) directly depends on tokenizer implementation
- [scripts/train_model.py](mdc:scripts/train_model.py) tightly coupled to specific tokenizer classes
- No clear separation between data processing, model, and training logic

### Poor Separation of Concerns
- Training logic mixed with model definition
- Data loading scattered across multiple files
- Configuration management not centralized

### Missing Abstractions
- No clear interfaces between components
- Hard to test individual components in isolation
- Difficult to swap implementations

## Architecture Improvement Plan

### 1. Dependency Injection Pattern

#### Tokenizer Interface
Create a clear interface for tokenizers:
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Union

class TokenizerInterface(ABC):
    @abstractmethod
    def encode(self, text: str) -> List[str]:
        pass
    
    @abstractmethod
    def encode_ids(self, text: str) -> List[int]:
        pass
    
    @abstractmethod
    def decode(self, tokens: Union[List[str], List[int]]) -> str:
        pass
    
    @property
    @abstractmethod
    def vocab_size(self) -> int:
        pass
```

#### Model Interface
Create a clear interface for models:
```python
class ModelInterface(ABC):
    @abstractmethod
    def forward(self, input_ids: torch.Tensor, **kwargs) -> torch.Tensor:
        pass
    
    @abstractmethod
    def generate(self, input_ids: torch.Tensor, **kwargs) -> torch.Tensor:
        pass
    
    @property
    @abstractmethod
    def config(self) -> ModelConfig:
        pass
```

### 2. Configuration Management

#### Centralized Configuration
Create a configuration system in [src/config.py](mdc:src/config.py):
```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class TokenizerConfig:
    vocab_size: int = 32000
    min_frequency: int = 2
    max_chars_per_token: int = 100
    special_tokens: Optional[List[str]] = None

@dataclass
class TrainingConfig:
    batch_size: int = 32
    learning_rate: float = 1e-4
    num_epochs: int = 100
    warmup_steps: int = 1000
    max_grad_norm: float = 1.0

@dataclass
class ModelConfig:
    vocab_size: int = 32000
    d_model: int = 768
    n_layers: int = 12
    n_heads: int = 12
    d_ff: int = 3072
    max_length: int = 512
    dropout: float = 0.1
```

### 3. Data Pipeline Abstraction

#### Dataset Interface
Create a unified dataset interface:
```python
class DatasetInterface(ABC):
    @abstractmethod
    def __len__(self) -> int:
        pass
    
    @abstractmethod
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        pass
    
    @abstractmethod
    def get_dataloader(self, **kwargs) -> DataLoader:
        pass
```

#### Data Processing Pipeline
Separate data processing logic:
```python
class DataProcessor:
    def __init__(self, tokenizer: TokenizerInterface, config: TokenizerConfig):
        self.tokenizer = tokenizer
        self.config = config
    
    def process_text(self, text: str) -> List[int]:
        """Process single text sample."""
        pass
    
    def process_batch(self, texts: List[str]) -> torch.Tensor:
        """Process batch of texts."""
        pass
```

### 4. Training Pipeline Abstraction

#### Trainer Interface
Create a generic trainer interface:
```python
class TrainerInterface(ABC):
    @abstractmethod
    def train(self, train_dataset: DatasetInterface, val_dataset: DatasetInterface) -> None:
        pass
    
    @abstractmethod
    def evaluate(self, dataset: DatasetInterface) -> Dict[str, float]:
        pass
    
    @abstractmethod
    def save_checkpoint(self, path: str) -> None:
        pass
    
    @abstractmethod
    def load_checkpoint(self, path: str) -> None:
        pass
```

#### Training Loop Separation
Separate training logic from model definition:
```python
class TrainingLoop:
    def __init__(self, model: ModelInterface, trainer: TrainerInterface):
        self.model = model
        self.trainer = trainer
    
    def run(self, config: TrainingConfig) -> None:
        """Run the training loop."""
        pass
```

## Implementation Strategy

### Phase 1: Create Interfaces
1. Define `TokenizerInterface` and `ModelInterface`
2. Create configuration classes
3. Update existing implementations to use interfaces

### Phase 2: Refactor Components
1. Refactor [src/model/transformer.py](mdc:src/model/transformer.py) to use dependency injection
2. Create separate data processing module
3. Refactor training scripts to use new interfaces

### Phase 3: Improve Testability
1. Create mock implementations for testing
2. Add integration tests for component interactions
3. Implement dependency injection container

## File Structure After Refactoring

```
src/
├── interfaces/
│   ├── __init__.py
│   ├── tokenizer.py      # TokenizerInterface
│   ├── model.py          # ModelInterface
│   ├── dataset.py        # DatasetInterface
│   └── trainer.py        # TrainerInterface
├── config/
│   ├── __init__.py
│   ├── base.py           # Base configuration classes
│   ├── tokenizer.py      # TokenizerConfig
│   ├── model.py          # ModelConfig
│   └── training.py       # TrainingConfig
├── data/
│   ├── __init__.py
│   ├── processor.py      # DataProcessor
│   ├── dataset.py        # Dataset implementations
│   └── loader.py         # DataLoader utilities
├── training/
│   ├── __init__.py
│   ├── trainer.py        # Trainer implementations
│   ├── loop.py           # TrainingLoop
│   └── utils.py          # Training utilities
├── tokenizer/
│   └── ...               # Tokenizer implementations
├── model/
│   └── ...               # Model implementations
└── utils/
    ├── __init__.py
    ├── logging.py        # Logging utilities
    └── metrics.py        # Metrics and evaluation
```

## Dependency Injection Container

### Simple DI Container
Create a dependency injection container:
```python
class Container:
    def __init__(self):
        self._services = {}
    
    def register(self, interface: type, implementation: type) -> None:
        self._services[interface] = implementation
    
    def resolve(self, interface: type) -> Any:
        if interface not in self._services:
            raise ValueError(f"Service {interface} not registered")
        return self._services[interface]()
```

### Usage Example
```python
# Register services
container = Container()
container.register(TokenizerInterface, WordPieceTokenizer)
container.register(ModelInterface, TransformerModel)

# Resolve dependencies
tokenizer = container.resolve(TokenizerInterface)
model = container.resolve(ModelInterface)
```

## Benefits of New Architecture

### Testability
- Easy to mock dependencies
- Components can be tested in isolation
- Clear interfaces for testing

### Maintainability
- Clear separation of concerns
- Easy to modify individual components
- Reduced coupling between modules

### Extensibility
- Easy to add new tokenizer implementations
- Easy to add new model architectures
- Easy to add new training strategies

### Reusability
- Components can be reused across different projects
- Clear interfaces make integration easier
- Configuration-driven behavior

## Migration Guidelines

### For Existing Code
1. Update imports to use new interfaces
2. Replace direct instantiation with dependency injection
3. Update configuration to use new config classes

### For New Code
1. Always use interfaces instead of concrete implementations
2. Use dependency injection for all dependencies
3. Follow the new file structure and naming conventions

### Backward Compatibility
- Maintain backward compatibility during transition
- Provide migration utilities
- Document breaking changes clearly
